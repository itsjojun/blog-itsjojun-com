---
title: How Computers work
date: 2025-09-22 17:04:06 +0200
categories: [Introduction into Computing for Beginners, Computing Basics]
tags: [computing basics]     # TAG names should always be lowercase
author: johannes
description: A brief and simple overview of how computers work.

# toc: false    # disable table of contents for this post
# comments: false   # disable commenting for this post
---

Computers don’t really understand words, pictures or videos the way humans do. In fact, they only really "understand" and therefore work with one thing: On/Off switches.

Sounds basic? It is (kinda)! But why switches you may ask? Well, let me explain. Every On/Off switch can be in two states. `On` or `Off`. One could also say `Yes` or `No`. Or `True` or `False`. Or simply put in numbers, `0` and `1`. 

One day in Germany,  a man called Gottfried Wilhelm Leibniz, a Mathematician who lived from 1646 to 1716, in the year 1703 written and 1705 published a paper called "Explication de l’arithmétique binaire [...]" (English: Explanation of the Binary Arithmetic)  in which he described his discoveries on the binary system and potential use cases for it. This system as (btw not newly discovered, but just refined and improved with more arthemic operations) described by him, which is a numerical system 

which uses only the characters 1 and 0, and some remarks on its usefulness.
https://en.wikipedia.org/wiki/Binary_code




Everything inside a computer boils down to these two states. And how does it do that? Well, it’s all thanks to tiny little components called **transistors**. A transistor is basically a super small switch. Each one can be turned **on** or **off**, just like a light switch on your wall. When it’s off, that’s a `0`. When it’s on, that’s a `1`.

You’ve probably seen this kind of switching in real life — think of the power button on your vacuum cleaner, or the ON/OFF switch on a lamp. It’s the same basic idea, just much, much smaller and much, much faster inside a computer.

Because computers can only work with these on/off states, everything we do on a computer — whether it's reading an email, watching a video, or listening to music — gets converted into **long sequences of 0s and 1s**. To the computer, that’s all data is: just millions (or billions) of tiny switches flipping between off and on, over and over.

This system of 0s and 1s is called the **binary system**. The name comes from the Latin word *bini*, which means “two together” or "a pair."

And because everything must be represented using just two options, computers need a *lot* of transistors to store and process all that information. Like, a *lot*. Modern processors contain **billions** of them, all packed into a chip no bigger than a fingernail. That’s what allows computers to do the massive amounts of work they do, from browsing the web to running 3D games.

Pretty fascinating, right? And it doesn’t stop there.

To make sense of those long strings of 0s and 1s, humans have come up with systems for encoding information. These systems define what specific combinations of bits mean. For example, in a widely used system called **ASCII**, the letter `'A'` is represented as `01000001`. The number `5` in binary is `00000101`.

These encodings are how we go from a bunch of electrical signals to things like text, images, videos, and apps. Without them, binary would just be meaningless noise.

> The word *computer* originally didn’t refer to a machine at all. It used to mean a **person** — someone whose job was to do math and calculations by hand. Over time, machines took over that job, and the word shifted to what we know today.
{: .prompt-info }

Finally, using only two states — on and off — has a big advantage. It makes computers **fast**, **reliable**, and **simple** at the hardware level. There’s less room for confusion, less chance for error, and the switching can happen at incredible speeds.

And that’s how computers — using nothing more than tiny switches and two basic states — do everything they do.
