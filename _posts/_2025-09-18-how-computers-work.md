---
title: How Computers work
date: 2025-09-22 17:04:06 +0200
categories: [Introduction into Computing for Beginners, Computing Basics]
tags: [computing basics]     # TAG names should always be lowercase
author: johannes
description: A brief and simple overview of how computers work.

# toc: false    # disable table of contents for this post
# comments: false   # disable commenting for this post
---

Computers don’t really understand words, pictures, or videos the way humans do. In fact, they don’t really "understand" anything at that all. In fact, at their core, computers only really "understand" and therefore work with just two basic things: **on** and **off**. That’s it. You can think of it like power being either **OFF** or **ON**, something or nothing, yes or no, false or true or simply **0** and **1**.

Everything inside a computer boils down to these two states. And how does it do that? Well, it’s all thanks to tiny little components called **transistors**. A transistor is basically a super small switch. Each one can be turned **on** or **off**, just like a light switch on your wall. When it’s off, that’s a `0`. When it’s on, that’s a `1`.

You’ve probably seen this kind of switching in real life — think of the power button on your vacuum cleaner, or the ON/OFF switch on a lamp. It’s the same basic idea, just much, much smaller and much, much faster inside a computer.

Because computers can only work with these on/off states, everything we do on a computer — whether it's reading an email, watching a video, or listening to music — gets converted into **long sequences of 0s and 1s**. To the computer, that’s all data is: just millions (or billions) of tiny switches flipping between off and on, over and over.

This system of 0s and 1s is called the **binary system**. The name comes from the Latin word *bini*, which means “two together” or "a pair."

And because everything must be represented using just two options, computers need a *lot* of transistors to store and process all that information. Like, a *lot*. Modern processors contain **billions** of them, all packed into a chip no bigger than a fingernail. That’s what allows computers to do the massive amounts of work they do, from browsing the web to running 3D games.

Pretty fascinating, right? And it doesn’t stop there.

To make sense of those long strings of 0s and 1s, humans have come up with systems for encoding information. These systems define what specific combinations of bits mean. For example, in a widely used system called **ASCII**, the letter `'A'` is represented as `01000001`. The number `5` in binary is `00000101`.

These encodings are how we go from a bunch of electrical signals to things like text, images, videos, and apps. Without them, binary would just be meaningless noise.

> The word *computer* originally didn’t refer to a machine at all. It used to mean a **person** — someone whose job was to do math and calculations by hand. Over time, machines took over that job, and the word shifted to what we know today.
{: .prompt-info }

Finally, using only two states — on and off — has a big advantage. It makes computers **fast**, **reliable**, and **simple** at the hardware level. There’s less room for confusion, less chance for error, and the switching can happen at incredible speeds.

And that’s how computers — using nothing more than tiny switches and two basic states — do everything they do.
